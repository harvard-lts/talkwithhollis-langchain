OPENAI_API_KEY=open_api_key
PRIMO_API_KEY=primo_api_key
PRIMO_API_HOST=https://fakehost.exlibrisgroup.com/primo/v1/search
PRIMO_API_LIMIT=100
HOLLIS_API_HOST=https://fakehost.harvard.edu/primo-explore/search
DIRECT_LINK_BASE_URL=https://example.com/path/to/permalink/
# Due to token limits when using context injection, we must limit the amount of primo results we send to the llm. This limit should be different for different llm models depending on their token capacity.
MAX_RESULTS_TO_LLM=5

# openai or azure or amazon
AI_PLATFORM=amazon

# Configures whether book lists are compiled by the llm or via python code
# Via the llm was the initial method, but since it's just manipulating json, this is more efficient with python code currently
# This functionality is preserved for now and configured off in case we want the llm to do something more with the data in the future
LLM_DO_RESPONSE_FORMATTING=false

# Profile name for AWS credentials
# AWS credentials
# https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html
AWS_ACCESS_KEY_ID=fake_access_key
AWS_SECRET_ACCESS_KEY=fake_secret_access_key
AWS_DEFAULT_REGION=us-east-1

# Profile name for AWS credentials (required if using credentials file)
#AWS_BEDROCK_PROFILE_NAME=example
AWS_BEDROCK_MODEL_ID=anthropic.claude-instant-v1

# If libcal library hours have not been refreshed in this many seconds, refresh them (1 day = 86400 seconds)
# LIBCAL_REFRESH_TIME=86400
LIBCAL_CLIENT_ID=1234
LIBCAL_CLIENT_SECRET=fake_libcal_secret
LIBCAL_TOKEN_API_ROUTE=https://example.com/token
LIBCAL_HOURS_API_ROUTE=https://example.com/hours
